#+TITLE:  Manipulation de données avec =pandas=
#+AUTHOR: Xavier Garrido
#+DATE:   08/02/2020
#+OPTIONS: toc:nil ^:{} author:nil
#+STARTUP:     beamer
#+LATEX_CLASS: python-slide
#+PROPERTY: header-args:jupyter-python :session tutorial_pandas

* =pandas= \faIcon{question-circle}

- =pandas= fournit des structures de données puissantes et simples à utiliser, ainsi que les moyens
  d'opérer rapidement des opérations sur ces structures

#+ATTR_BEAMER: :overlay +-
- Installation /via/ =pip=
  #+BEGIN_SRC shell-session
    @\prompt@ pip install pandas
  #+END_SRC

- Convention d'importation
  #+BEGIN_SRC python
    In [1]: import pandas as pd
  #+END_SRC
* =pandas= \faIcon{question-circle}

- =pandas= est à =numpy= ce que les dictionnaires sont aux listes en Python

- Pour rappel, l'accès aux éléments d'une liste ou d'un tableau =numpy= se fait par indice

  #+BEGIN_SRC python
    In [1]: l = [0, 1, 2, 3]
    In [2]: l[0]
    Out[2]: 0
  #+END_SRC

#+BEAMER: \pause
- Pour un dictionnaire, l'accès se fait par clé

  #+BEGIN_SRC python
    In [1]: d = {"a": 0, "b": 1, "c": 2, "d": 3}
    In [2]: d["a"]
    Out[2]: 0
  #+END_SRC

- =pandas= généralise l'utilisation de valeurs autres qu'entières pour l'accès aux données numériques

* =pandas.Series=

- L'objet =Series= généralise la déclaration de *tableau =numpy= à 1 dimension* en associant un indice ou
  */index/* à chaque ligne du tableau

  #+ATTR_LATEX: :options fontsize=\scriptsize
  #+BEGIN_SRC python
    In [1]: data = pd.Series([0, 1, 2, 3], index=["a", "b", "c", "d"])
    In [2]: data
    Out[2]:
    a    0
    b    1
    c    2
    d    3
    dtype: int64

    In [3]: data.index
    Out[3]: Index(['a', 'b', 'c', 'd'], dtype='object')

    In [4]: data.values
    Out[4]: array([0, 1, 2, 3])

    In [5]: data["a"]
    Out[5]: 0
  #+END_SRC

* =pandas.Series= : création & parcours

  #+ATTR_LATEX: :options fontsize=\scriptsize
  #+BEGIN_SRC python
    In [1]: population_dict = {"California": 38332521,
                               "Texas": 26448193,
                               "New York": 19651127,
                               "Florida": 19552860,
                               "Illinois": 12882135}
    In [2]: population = pd.Series(population_dict)
    In [3]: population
    Out[3]:
    California    38332521
    Texas         26448193
    New York      19651127
    Florida       19552860
    Illinois      12882135
    dtype: int64

    In [4]: population["Texas"], population.Texas
    Out[4]: (26448193, 26448193)

    In [5]: population["Texas":"Florida"]
    Out[5]:
    Texas       26448193
    New York    19651127
    Florida     19552860
    dtype: int64
  #+END_SRC

* =pandas.DataFrame=

- L'objet =DataFrame= généralise la déclaration de *tableau =numpy= à 2 dimensions* en associant à la fois un
  indice ou */index/* pour chaque ligne du tableau de même qu'un nom pour chaque *colonne*

  #+ATTR_LATEX: :options fontsize=\scriptsize
  #+BEGIN_SRC python
    In [6]: area_dict = {"California": 423967, "Texas": 695662, "New York": 141297,
                         "Florida": 170312, "Illinois": 149995}
    In [7]: area = pd.Series(area_dict)

    In [8]: states = pd.DataFrame({"population": population,
                                   "area": area})
    In [9]: states
    Out[9]:
                population    area
    California    38332521  423967
    Texas         26448193  695662
    New York      19651127  141297
    Florida       19552860  170312
    Illinois      12882135  149995

    In [10]: states.index
    Out[10]: Index(['California', 'Texas', 'New York', 'Florida', 'Illinois'], dtype='object')

    In [11]: states.columns
    Out[11]: Index(['population', 'area'], dtype='object')
  #+END_SRC

* =pandas.DataFrame= : création & parcours

- =DataFrame= \to matrice =numpy=
  #+ATTR_LATEX: :options fontsize=\scriptsize
  #+BEGIN_SRC python
    In [12]: states.values
    Out[12]:
    array([[38332521,   423967],
           [26448193,   695662],
           [19651127,   141297],
           [19552860,   170312],
           [12882135,   149995]])
  #+END_SRC

#+BEAMER: \pause
- Parcours par « indice »
  #+ATTR_LATEX: :options fontsize=\scriptsize
  #+BEGIN_SRC python
    In [13]: states["population"]
    Out[13]:
    California    38332521
    Texas         26448193
    New York      19651127
    Florida       19552860
    Illinois      12882135
    Name: population, dtype: int64

    In [14]: states["population"]["Texas"]
    Out[14]: 26448193
  #+END_SRC

#+BEGIN_REMARK
Contrairement aux tableaux =numpy=, l'accès se fait d'abord par colonne puis par ligne !
#+END_REMARK

* =pandas.DataFrame= : création & parcours

- Parcours /à la/ =numpy= : =iloc=, =loc=
  #+ATTR_LATEX: :options fontsize=\scriptsize
  #+BEGIN_SRC python
    In [15]: states.iloc[:1, :1]
    Out[15]:
                population
    California    38332521

    In [16]: states.loc["Texas"]
    Out[16]:
    population    26448193
    area            695662
    Name: Texas, dtype: int64
  #+END_SRC

* =pandas.DataFrame= : création & parcours

- Ajout de données
  #+ATTR_LATEX: :options fontsize=\scriptsize
  #+BEGIN_SRC python
    In [17]: states["density"] = states["population"]/states["area"]
    In [18]: states
    Out[18]:
                population    area     density
    California    38332521  423967   90.413926
    Texas         26448193  695662   38.018740
    New York      19651127  141297  139.076746
    Florida       19552860  170312  114.806121
    Illinois      12882135  149995   85.883763

    In [19]: states.T
    Out[19]:
                  California         Texas  ...       Florida      Illinois
    population  3.833252e+07  2.644819e+07  ...  1.955286e+07  1.288214e+07
    area        4.239670e+05  6.956620e+05  ...  1.703120e+05  1.499950e+05
    density     9.041393e+01  3.801874e+01  ...  1.148061e+02  8.588376e+01

    [3 rows x 5 columns]
  #+END_SRC

* =pandas= par l'exemple

- Il existe pléthore de ressources sur internet
  - documentation officielle de [[https://pandas.pydata.org/pandas-docs/stable/pandas.pdf][=pandas=]] (3021 pages !)
  - /Python Data Science Handbook/ et son chapitre sur [[https://jakevdp.github.io/PythonDataScienceHandbook/03.00-introduction-to-pandas.html][=pandas=]]
  - multitudes de tutoriels en ligne :
    - [[https://pandas.pydata.org/pandas-docs/stable/getting_started/tutorials.html][tutoriels officiels =pandas=]] dont [[https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html][10 minutes to =pandas=]],
    - [[https://openclassrooms.com/fr/courses/4452741-decouvrez-les-librairies-python-pour-la-data-science/5558996-passez-de-numpy-a-pandas][Open Class Rooms]]
    - ...

- Tutoriel intéractif \to =jupyter= /notebook/
  -
  -
  - depuis votre terminal /via/ la commande

    #+BEGIN_SRC shell-session
    @\prompt@ jupyter lab /chemin/vers/le/notebook
    #+END_SRC

#+BEGIN_SRC latex
  \begin{textblock}{}(2.75,8.7)
    \href{https://colab.research.google.com/github/xgarrido/master_python_teaching/blob/master/slides/04_slide_pandas.ipynb}{\includegraphics[height=0.8em]{./figures/colab-badge.png}}
   \end{textblock}

  \begin{textblock}{}(2.75,9.35)
    \href{https://mybinder.org/v2/gh/xgarrido/master_python_teaching/master?filepath=slides\%2F04_slide_pandas.ipynb}{\includegraphics[height=0.8em]{./figures/launch-binder.png}}
   \end{textblock}
#+END_SRC

#+RESULTS:
#+begin_export latex
\begin{textblock}{}(2.75,8.7)
  \href{https://colab.research.google.com/github/xgarrido/master_python_teaching/blob/master/slides/04_slide_pandas.ipynb}{\includegraphics[height=0.8em]{./figures/colab-badge.png}}
 \end{textblock}

\begin{textblock}{}(2.75,9.35)
  \href{https://mybinder.org/v2/gh/xgarrido/master_python_teaching/master?filepath=slides\%2F04_slide_pandas.ipynb}{\includegraphics[height=0.8em]{./figures/launch-binder.png}}
 \end{textblock}
#+end_export

* pandas par l'exemple                                             :noexport:
** Analyse des prénoms parisiens depuis 2004
*** Lecture & importation de fichier de données
Commençons par l'importation du module =pandas=
#+BEGIN_SRC jupyter-python :results none
  import pandas as pd
#+END_SRC

Nous allons nous intéresser dans ce /notebook/ à l'historique des prénoms déclarés à l'état-civil de
2004 à 2019 sur la ville de Paris. Ces données sont téléchargeables depuis le site
[[https://data.gouv.fr][https://data.gouv.fr]] et, pour le fichier qui nous intéresse, à cette [[https://www.data.gouv.fr/fr/datasets/r/30800be0-8b72-4e89-9ecf-58ea7dedfe86][adresse]]. Une fois le fichier
téléchargé, nous pouvons jeter un coup d'oeil aux premières lignes
#+BEGIN_SRC jupyter-python
  !head data/liste_des_prenoms.csv
#+END_SRC

#+RESULTS:
: Nombre;Sexe;Annee;Prenoms;Nombre total cumule par annee
: 7;M;2013;Aydan;7
: 6;F;2013;Béatrice;6
: 6;M;2013;Boubacar;6
: 5;M;2013;Camilo;5
: 14;M;2013;Charly;14
: 6;F;2013;Chayma;6
: 67;M;2013;David;67
: 6;F;2013;Delia;6
: 6;F;2013;Eleanor;6

Le fichier brut présente ainsi 5 colonnes dont l'intitulé se trouve à la première ligne. Nous allons
charger ce fichier dans un objet de type =pandas.DataFrame= afin de pouvoir le manipuler
#+BEGIN_SRC jupyter-python :results none
  data = pd.read_csv("./data/liste_des_prenoms.csv", sep=";")
#+END_SRC

Les commandes =head()/tail()= permettent d'avoir un aperçu des premières/dernières lignes du =DataFrame=
#+BEGIN_SRC jupyter-python :results none
  data.head()
#+END_SRC

Les indices et les colonnes sont
#+BEGIN_SRC jupyter-python :results none
  data.index, data.columns
#+END_SRC
tandis que la "forme" et le type de données sont accessibles /via/
#+BEGIN_SRC jupyter-python :results none
  data.shape, data.dtypes
#+END_SRC
*** Premier analyse & manipulation des données
Dans un premier temps, nous allons renommer la colonne "Nombre total cumule par annee" en "Cumul"
afin de pouvoir accéder à cette colonne plus facilement
#+BEGIN_SRC jupyter-python :results none
  data.rename(columns={"Nombre total cumule par annee": "Cumul"}, inplace=True)
#+END_SRC

À la lecture des premières lignes, on peut également se poser la question de la différence entre les
données de la colonne "Nombre" et "Cumul". Nous allons dénombrer les lignes pour lesquels ces deux
valeurs sont différentes
#+BEGIN_SRC jupyter-python
  import numpy as np
  mask = (data.Cumul - data.Nombre != 0)
  print(np.sum(mask), "sur un total de", data.shape[0], "lignes")
#+END_SRC

#+RESULTS:
: 232 sur un total de 20453 lignes

puis afficher ces lignes pour tenter de comprendre l'origine de la différence
#+BEGIN_SRC jupyter-python :results none
  data[mask]
#+END_SRC
Il semble que les différences sont dues aux prénoms mixtes. Affichons les lignes associées au prénom
"Camille"
#+BEGIN_SRC jupyter-python :results none
  data[data.Prenoms == "Camille"]
#+END_SRC

On constate bien que la valeur cumulée correspond à la somme par année des occurences de "Camille"
garçons comme filles. Par la suite, nous allons traiter chaque population de façon distincte, nous
pouvons donc supprimer cette colonne
#+BEGIN_SRC jupyter-python
  data.pop("Cumul")
#+END_SRC

Une fois cette première sélection réalisée, nous pouvons avoir un rapide aperçu des propriétés
statistiques du fichier et de chaque colonne à l'aide de la fonction =describe()=
#+BEGIN_SRC jupyter-python :results none
  data.describe()
#+END_SRC

La valeur moyenne du nombre d'occurences d'un prénom est ainsi voisine de ~25 tandis que la valeur
médiane est à 11. La distribution du nombre d'occurences est ainsi totalement asymétrique comme nous
pouvons le voir en représentant la distribution de ces valeurs
#+BEGIN_SRC jupyter-python
  data.Nombre.plot.hist(log=True, bins=30);
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/e58e30c5c00d1586c6ce48a8b59ab8a336602364.png]]

*** Aggrégation & tri de valeurs

Les données fournis par le fichier sont désordonnées aussi bien du point de vue des années que des
prénoms. Nous pouvons trier ces données en les ordonnant temporellement à l'aide de la fonction
=sort_values()=
#+BEGIN_SRC jupyter-python :results none
  data.sort_values(by="Annee")
#+END_SRC

L'argument =by= peut tout à la fois prendre le nom du colonne ou une liste de noms de colonnes afin de
réaliser un tri successif selon chaque colonne. Ainsi pour trier par année puis, de façon
décroissante, par nombre, nous pouvons écrire
#+BEGIN_SRC jupyter-python :results none
  data.sort_values(by=["Annee", "Nombre"], ascending=False)
#+END_SRC

On peut également grouper les valeurs selon une colonne. La fonction =groupby()= permet, par exemple,
de regrouper les valeurs par année et d'extraire par la suite le nombre total de naissance par année
#+BEGIN_SRC jupyter-python
  data.groupby("Annee").sum().plot(style="--o");
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/cddb5ae60954d3c34c4a86a4e522419557556d92.png]]
On peut également regrouper les naissances en fonction de l'année et du sexe des enfants
#+BEGIN_SRC jupyter-python
  data.groupby(["Annee", "Sexe"]).sum()
#+END_SRC

En groupant les données selon deux catégories (Année, Sexe), on obtient un nouvel objet =DataFrame=
dont les indices sont des couples (Année, Sexe)
#+BEGIN_SRC jupyter-python
  data1 = data.groupby(["Annee", "Sexe"]).sum()
  data1.index
#+END_SRC

La fonction =unstack()= permet alors de transformer cet objet afin de disposer du nombre de naissances
par sexe en fonction de l'année
#+BEGIN_SRC jupyter-python
  data1.unstack()
#+END_SRC

#+BEGIN_SRC jupyter-python
  data1.unstack().plot(style="--o")
#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.axes._subplots.AxesSubplot at 0x7fa7b33eea90>
[[file:./.ob-jupyter/3e83a7ad2fec583d0c5f998c48388fccf40d6435.png]]
:END:

Finalement, pour trouver les noms les plus utilisés en fonction des années et du sexe de l'enfant,
on peut utiliser la fonction =max()= qui retourne, après avoir groupé les données, les valeurs
maximales *par colonne*
#+BEGIN_SRC jupyter-python
  data.groupby(["Annee", "Sexe"]).max()
#+END_SRC

Les prénoms affichés correspondent aux valeurs "maximales" soit, pour des chaines de caractères, au
tri alphabétique : Zinédine n'est pas le prénom le plus utilisé en 2004. En revanche, le nombre
maximal de fois où un prénom a été donné correspond bien aux valeurs portées par la colonne
"Nombre". Il s'agit donc pour ces valeurs de trouver les prénoms associés. On peut alors chercher
les indices correspondant à ces valeurs maximales /via/ la fonction =idxmax()=
#+BEGIN_SRC jupyter-python
  data.groupby(["Annee", "Sexe"]).idxmax()
#+END_SRC
puis d'afficher ces lignes
#+BEGIN_SRC jupyter-python
  data.iloc[data.groupby(["Annee", "Sexe"]).idxmax()["Nombre"]]
#+END_SRC

*** Pivoter les données

Il est possible de transposer les données de telle sorte à inverser indices et colonnes. On peut
également utiliser la fonction =pivot_table()= pour déterminer la façon dont va pivoter le
=DataFrame=. Cette fonction prend 3 arguments :

- =values= indique les valeurs selon lesquelles réaliser le pivot de la table de données,
- =columns= permet de spécifier les valeurs qui deviendront les colonnes dans la nouvelle table,
- =index= permet de spécifier ce qui deviendra l'index de la table pivotée.

On peut, par exemple, représenter le nombre de prénoms attribués (=values=) en fonction de l'année
(=columns=) pour chaque prénoms (=index=)
#+BEGIN_SRC jupyter-python
  data.pivot_table(values="Nombre", index="Prenoms", columns="Annee")
#+END_SRC

On note qu'en l'absence de valeurs, =pandas= associe la valeur /Not a Number/, valeur que l'on peut
ignorer =dropna()= ou que l'on peut modifier de la façon suivante
#+BEGIN_SRC jupyter-python
  data.pivot_table(values="Nombre", index="Prenoms", columns="Annee").fillna(0)
#+END_SRC

Nous avons ainsi accès à la tendance annuelle d'un prénom en le sélectionnant
#+BEGIN_SRC jupyter-python
  data.pivot_table(values="Nombre", index="Prenoms", columns="Annee").fillna(0).loc[["Patrick", "Pascal"]].astype(int)
#+END_SRC

Le résultat graphique est encore plus parlant
#+BEGIN_SRC jupyter-python
  data.pivot_table(values="Nombre", index="Prenoms", columns="Annee").fillna(0).loc[["Patrick", "Pascal"]].T.plot.bar();
#+END_SRC
** Analyse, tri et manipulation de données temporelles
=pandas= permet de manipuler des données temporelles en facilitant notamment la lecture des dates,
heures, ... Nous allons dans cette seconde partie étudier la distribution temporelle correspondant à
la traversée du pont Fremont à Seattle en vélo. Ce notebook est une application directe du chapitre
[[https://jakevdp.github.io/PythonDataScienceHandbook/03.11-working-with-time-series.html#Example:-Visualizing-Seattle-Bicycle-Counts]["/Working with time series/"]].

Les données relevées par la ville de Seattle sont consultables à cette [[https://data.seattle.gov/Transportation/Fremont-Bridge-Bicycle-Counter/65db-xm6k][adresse]]. Nous pouvons
directement les charger dans =DataFrame= à l'aide la fonction =read_csv()= à laquelle nous indiquons
utiliser les dates de relevés comme indices
#+BEGIN_SRC jupyter-python :results none
  data = pd.read_csv("https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD", parse_dates=True, index_col="Date")
  data.head()
#+END_SRC

Les données donne la fréquence horaire de traversée d'Est en Ouest du pont Fremont. Afin de
faciliter la manipulation des données nous allons renommer chaque colonne
#+BEGIN_SRC jupyter-python
  data.columns = ["Total", "East", "West"]
#+END_SRC

# Vérifier s'il y a des =nan=
# #+BEGIN_SRC jupyter-python
#   data.isna().values.any()
# #+END_SRC

# #+RESULTS:
# : True

*** Visualisation des données

Afin d'avoir un rapide aperçu statistique, on peut toujours utiliser la fonction =describe()=
#+BEGIN_SRC jupyter-python :results none
  data.dropna().describe()
#+END_SRC
ou visualiser le nombre de traversée en fonction de l'heure
#+BEGIN_SRC jupyter-python
  data.plot();
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/d51af58d8663d1465a2123411cecfe520f6901ce.png]]

Le taux horaire de données est tel qu'il convient de rééchantillonner les valeurs par semaine
#+BEGIN_SRC jupyter-python
  weekly = data.resample("W").sum()
  weekly.plot().set_ylabel("Nombre de traversées par semaine");
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/7cbc0bc734cfc445da26d1005762d74059b8f6be.png]]

Les données laissent clairement apparaître des variations été/hiver de même qu'une augmentation du
trafic depuis l'ouest vers l'est depuis 2017.

Nous pouvons également lisser ces courbes à l'aide d'une moyenne glissante sur la base d'un
échantillonnage journalier. On utilise à cette fin une moyenne glissante sur 30 jours à l'aide de la
fonction =rolling()=
#+BEGIN_SRC jupyter-python
  daily = data.resample("D").sum()
  daily.rolling(30, center=True).mean().plot().set_ylabel("Nombre moyen de traversée par jour");
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/700c3da1b91df092e34fb8242cc4ba732fd2308a.png]]

On peut finalement faire ce même exercice en utilisant une fonction gaussienne afin de minimiser
encore les variations (/Kernel Density Estimation/)
#+BEGIN_SRC jupyter-python
  daily.rolling(50, center=True, win_type="gaussian").mean(std=10).plot().set_ylabel("Nombre moyen de traversée par jour");
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/a685575ac8cda2f8ccb18680d84ce8ccb742c7b3.png]]
*** Variation du trafic par heure
À l'image des prénoms, il est possible de grouper les données par heure afin d'estimer les
variations moyennes au cours d'une journée
#+BEGIN_SRC jupyter-python :results none
  by_time = data.groupby(data.index.time)
  by_time.describe()
#+END_SRC

et de représenter graphiquement ces données
#+BEGIN_SRC jupyter-python
  hourly_ticks = 4 * 60 * 60 * np.arange(6)
  by_time.mean().plot(xticks=hourly_ticks);
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/fb96b31fc0fe68dd2a1cedb0ff41a8a41f51f9dc.png]]

La distribution est bimodale est fonction des heures de travail (8h le matin, 17h l'après midi) et
les traversées s'inversent bien (la direction est/ouest correspond au trafic agglomération vers
centre-ville)

Nous pouvons également nous intéresser à la fréquence de traversée en fonction du jour et vérifier
que l'utilisation du vélo se fait majoritairement pour se rendre au travail
#+BEGIN_SRC jupyter-python
  by_weekday = data.groupby(data.index.dayofweek).mean()
  by_weekday.index = ["Lundi", "Mardi", "Merc.", "Jeudi", "Vend.", "Samedi", "Dim."]
  by_weekday.plot();
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/c3334b59a0a12bb835f1e65fc70beff2ec328da0.png]]

Finalement, nous pouvons grouper les deux informations heure et jour et comparer les distributions
en fonction du jour de la semaine
#+BEGIN_SRC jupyter-python
  weekend = np.where(data.index.weekday < 5, "Semaine", "Week-end")
  by_time = data.groupby([weekend, data.index.time]).mean()

  import matplotlib.pyplot as plt
  fig, ax = plt.subplots(1, 2, figsize=(10,5))
  by_time.loc["Semaine"].plot(ax=ax[0], title="Semaine", xticks=hourly_ticks)
  by_time.loc["Week-end"].plot(ax=ax[1], title="Week-end", xticks=hourly_ticks);
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/ef72c93fb302bab2ea47b07f170033e20ec4bab9.png]]
